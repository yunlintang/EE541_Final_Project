{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "edb80a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os.path import exists\n",
    "\n",
    "from ast import literal_eval\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4048821",
   "metadata": {},
   "source": [
    "# Preprocess Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d7cd562",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\yunli\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\yunli\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# download necessary packages\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "sw = stopwords.words('english')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def remove_stop(x):\n",
    "    '''\n",
    "    function to remove the stopwords(English) from the\n",
    "    input list of string\n",
    "    \n",
    "    Params:\n",
    "        x: a list of string (ex. ['a','b','c'])\n",
    "        \n",
    "    Returns:\n",
    "        returns a new a list of string\n",
    "    '''\n",
    "    try:\n",
    "        new_list = [i for i in x.split(\" \") if i not in sw ]\n",
    "    except:\n",
    "        return np.nan\n",
    "    return new_list\n",
    "\n",
    "\n",
    "def lemma(x):\n",
    "    '''\n",
    "    function to lemmatize the input(a list of string)\n",
    "    \n",
    "    Params:\n",
    "        x: a list of string (ex. ['a','b','c'])\n",
    "        \n",
    "    Returns:\n",
    "        a string the combine all strings in the result list\n",
    "        (ex. \"a b c\")\n",
    "    '''\n",
    "    if x is np.nan:\n",
    "        return \"\"\n",
    "    new_list = []\n",
    "    for i in x:\n",
    "        new_list += [lemmatizer.lemmatize(i)]\n",
    "    \n",
    "    return ' '.join(new_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289b6b67",
   "metadata": {},
   "source": [
    "# Text Vectorizers (5 types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ebde4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf(df,var,mdf=1,mfeatures=None,return_vec=False):\n",
    "    '''\n",
    "    function to vectorize the text feature(ex. \"a b c\")\n",
    "    into numeric vector by using the TF-IDF method.\n",
    "    Note: can be modified to reduce dimension(use \"min_df\" and \"max_features\")\n",
    "    \n",
    "    Params:\n",
    "        df: input dataset\n",
    "        var: text feature that need to be vectorized\n",
    "        mdf: min_df\n",
    "        mfeatures: max_features\n",
    "        return_vec: if true, return the fitted vectorizer\n",
    "        \n",
    "    Returns:\n",
    "        the vectorized text feature(vector of numeric vectors)\n",
    "    '''\n",
    "    vectorizer = TfidfVectorizer(stop_words='english',\n",
    "                                 min_df=mdf, max_features=mfeatures)\n",
    "    vec = vectorizer.fit_transform(df[var])\n",
    "    \n",
    "    # Note: to vectorize a unseen vector(a string), use:\n",
    "    # model.transform()\n",
    "    if return_vec:\n",
    "        return (vectorizer, vec.toarray())\n",
    "    return vec.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c452cbed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hashing(df,var,n=500,return_vec=False):\n",
    "    '''\n",
    "    function to vectorize the text feature(ex. \"a b c\")\n",
    "    into numeric vector by using the hashing method.\n",
    "    Note: can be modified to reduce dimension(use \"n_features\")\n",
    "    \n",
    "    Params:\n",
    "        df: input dataset\n",
    "        var: text feature that need to be vectorized\n",
    "        n: n_features\n",
    "        return_vec: if true, return the fitted vectorizer\n",
    "        \n",
    "    Returns:\n",
    "        the vectorized text feature(vector of numeric vectors)\n",
    "    '''\n",
    "    vectorizer = HashingVectorizer(stop_words='english', n_features=n)\n",
    "    vec = vectorizer.transform(df[var])\n",
    "    \n",
    "    # Note: to vectorize a unseen vector(a string), use:\n",
    "    # model.transform()\n",
    "    if return_vec:\n",
    "        return (vectorizer, vec.toarray())\n",
    "    return vec.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4b0120e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nnemb(df,var,d):\n",
    "    '''\n",
    "    function to vectorize the input text feature(ex. \"a b c\") by\n",
    "    using the word embedding method from PyTorch\n",
    "    Note: compute meach word embedding vecor for each input text \n",
    "    then use it as the numeric representaion of the input text\n",
    "    \n",
    "    Params:\n",
    "        df: the input dataset\n",
    "        var: the feature name\n",
    "        d: dimension want to be kept\n",
    "        \n",
    "    Returns:\n",
    "        return the vectorized text\n",
    "    '''\n",
    "    # use countvectorizer to compute the vocab. for the input\n",
    "    vectorizer = CountVectorizer()\n",
    "    vectorizer.fit(df[var])\n",
    "    vocab = vectorizer.vocabulary_\n",
    "    # construct the word embedding model\n",
    "    embeds = nn.Embedding(len(vocab),d)\n",
    "    \n",
    "    def ConvertToVec(x):\n",
    "        '''\n",
    "        function to vectorize the single input of string\n",
    "        '''\n",
    "        # function to preprocess the string by the countvectorizer\n",
    "        t = vectorizer.build_analyzer()\n",
    "        # generate tensors for each word in the input string by\n",
    "        # indexing the vocab\n",
    "        lookup_tensor = torch.tensor([vocab[i] for i in t(x)],dtype=torch.long)\n",
    "        # compute the numeric vector\n",
    "        x_vec = embeds(lookup_tensor)\n",
    "        x_vec = torch.mean(x_vec,axis=0)\n",
    "        return x_vec\n",
    "    \n",
    "    return df[var].apply(ConvertToVec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d68a17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def senbert(df,var,model_name,params=None,return_vec=False):\n",
    "    '''\n",
    "    function to vectorize the input text feature by using the\n",
    "    selected Sentence-BERT model.\n",
    "    (models can be found on:\n",
    "    https://docs.google.com/spreadsheets/d/14QplCdTCDwEmTqrn1LH4yrbKvdogK4oQvYO1K1aPR5M/edit#gid=0)\n",
    "    \n",
    "    Params:\n",
    "        df: input dataset\n",
    "        var: the name of text feature\n",
    "        model_name: name of model\n",
    "        params: params for the model\n",
    "        return_vec: if true, return the fitted vectorizer\n",
    "        \n",
    "    Return:\n",
    "        the vectorized feature text\n",
    "    '''\n",
    "    model = SentenceTransformer(model_name)\n",
    "    if params is not None:\n",
    "        params['sentences'] = df[var]\n",
    "        X = model.encode(**params)\n",
    "    else:\n",
    "        X = model.encode(df[var],batch_size=10)\n",
    "        \n",
    "    # return the trained model\n",
    "    # Note: to vectorize a unseen vector(a string), use:\n",
    "    # model.encode()[0]\n",
    "    if return_vec:\n",
    "        return (model,X)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "695518e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc2vec(df,var,d,return_vec=False):\n",
    "    '''\n",
    "    function to convert the input text feature by using\n",
    "    the Doc2Vec model by Gensim.\n",
    "    \n",
    "    Params:\n",
    "        df: input dataset\n",
    "        var: the name of text feature\n",
    "        d: dimension of feature to be kept\n",
    "        return_vec: if true, return the fitted vectorizer\n",
    "        \n",
    "    Returns:\n",
    "        \n",
    "    '''\n",
    "    sen = [TaggedDocument(sen,[i]) for i,sen in enumerate(df[var].values)]\n",
    "    model = Doc2Vec(sen,vector_size=d)\n",
    "    X = [model.dv[i] for i in range(len(df[var]))]\n",
    "    \n",
    "    # return the trained model\n",
    "    # Note: to vectorize a unseen vector(a list of strings), use:\n",
    "    # model.infer_vector()\n",
    "    if return_vec:\n",
    "        return (model,X)\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db1a7c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: valid./test separate from the build model(vocab)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7edacd68",
   "metadata": {},
   "source": [
    "# Combine Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "652c4b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(df,vectpath):\n",
    "    # save senbert vectorized vectors for overview\n",
    "    X_senbert = senbert(df,'overview','stsb-distilroberta-base-v2')\n",
    "    np.savetxt(vectpath+\"overview_senbert.txt\", X_senbert,delimiter=',')\n",
    "\n",
    "    # save Doc2Vec vectorized vectors for overview\n",
    "    X_doc2vec = doc2vec(df,'overview',500)\n",
    "    np.savetxt(vectpath+\"overview_doc2vec.txt\", X_doc2vec,delimiter=',')\n",
    "\n",
    "    # save hashing vectorized vectors for overview\n",
    "    X_hash = hashing(df,'overview',500)\n",
    "    np.savetxt(vectpath+\"overview_hash.txt\", X_hash,delimiter=',')\n",
    "    \n",
    "    # save senbert vectorized vectors for title\n",
    "    X_senbert_t = senbert(df,'title','stsb-distilroberta-base-v2')\n",
    "    np.savetxt(vectpath+\"title_senbert.txt\", X_senbert_t,delimiter=',')\n",
    "\n",
    "    # save Doc2Vec vectorized vectors for title\n",
    "    X_doc2vec_t = doc2vec(df,'title',500)\n",
    "    np.savetxt(vectpath+\"title_doc2vec.txt\", X_doc2vec_t,delimiter=',')\n",
    "\n",
    "    # save hashing vectorized vectors for title\n",
    "    X_hash_t = hashing(df,'title',500)\n",
    "    np.savetxt(vectpath+\"title_hash.txt\", X_hash_t,delimiter=',')\n",
    "    \n",
    "    print(\"all vectorized text features are saved in {}\".format(vectpath))\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "273ef782",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Combine_Features(inpath,vectpath,title_,overview_,outpath,save=False):\n",
    "    # read the combine data after one-hot enginnering\n",
    "    df = pd.read_csv(inpath+'combine_clean_oh.csv')\n",
    "    \n",
    "    # preprocess the text features\n",
    "    df['title'] = df['title'].apply(remove_stop).apply(lemma)\n",
    "    df['overview'] = df['overview'].apply(remove_stop).apply(lemma)\n",
    "    \n",
    "    # vectorize the text features if not exsited\n",
    "    if not (exists(vectpath+title_) and exists(vectpath+overview_)):\n",
    "        vectorize(df,vectpath)\n",
    "        \n",
    "    # read the vectorized text features(\"overview\" and \"title\")\n",
    "    df_overview = pd.read_csv(vectpath+overview_,header=None).add_prefix(\"Overview_\")\n",
    "    df_title = pd.read_csv(vectpath+title_,header=None).add_prefix(\"title_\")\n",
    "    \n",
    "    # combine the dataset\n",
    "    df_comb = pd.concat([df,df_overview,df_title],axis=1)\n",
    "    # drop features and move score to the last column\n",
    "    df_comb['new_score'] = df_comb['score']\n",
    "    df_comb = df_comb.drop(columns=['overview','title','keywords','score','id'])\n",
    "    df_comb = df_comb.rename(columns={'new_score':'score'})\n",
    "    \n",
    "    # save\n",
    "    if save:\n",
    "        filename = \"data.csv\"\n",
    "        df_comb.to_csv(outpath+filename,index=False)\n",
    "        print(\"the final dataset after all feature enginnerings is saved in {}\".format(outpath))\n",
    "        \n",
    "    return df_comb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61984d54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af02c1bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ddf9352",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b330e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48cccbe3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
